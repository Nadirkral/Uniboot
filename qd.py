# ================================
# portrait_draw_ai.py - Part 1
# Ultra-Enhanced Portrait Drawing AI
# Developed by Nadir & ChatGPT
# ================================

# ğŸ“š Import libraries
import os
import cv2
import time
import math
import json
import shutil
import random
import logging
import argparse
import numpy as np
from datetime import datetime
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt

# ğŸ§  Deep learning for face and landmarks
import dlib

# ğŸ’¬ Optional: GUI for user interaction
import tkinter as tk
from tkinter import filedialog, messagebox

# ğŸ“ Create necessary directories
def create_directories():
    os.makedirs("outputs/images", exist_ok=True)
    os.makedirs("outputs/frames", exist_ok=True)
    os.makedirs("outputs/videos", exist_ok=True)
    os.makedirs("outputs/pdf", exist_ok=True)
    os.makedirs("logs", exist_ok=True)

# ğŸªµ Setup logging
def setup_logging():
    log_path = f"logs/log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    logging.basicConfig(
        filename=log_path,
        level=logging.DEBUG,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    logging.info("==== Yeni sessiya baÅŸlandÄ± ====")

# ğŸ§¾ Konfiqurasiya (daha sonra fayla yaza bilÉ™rik)
CONFIG = {
    "frame_delay": 0.05,
    "video_fps": 30,
    "line_thickness": 1,
    "ai_style_transfer": True,
    "face_detection_model": "mmod_human_face_detector.dat",  # É™gÉ™r mÃ¶vcuddursa
    "shape_predictor_model": "shape_predictor_68_face_landmarks.dat"
}

# ğŸ¯ Setup everything
def initialize():
    print("[âœ”] Qovluqlar yaradÄ±lÄ±r...")
    create_directories()
    print("[âœ”] Log sistemi aktiv edildi.")
    setup_logging()
    logging.info("Konfiqurasiya: %s", json.dumps(CONFIG, indent=2))

if __name__ == "__main__":
    initialize()
    # ================================
# Part 2: Image Load & Face Detection
# ================================

# ğŸ“¤ ÅÉ™klin seÃ§ilmÉ™si (GUI ilÉ™ vÉ™ ya CLI ilÉ™)
def select_image_gui():
    root = tk.Tk()
    root.withdraw()
    file_path = filedialog.askopenfilename(
        title="ÅÉ™kil seÃ§in",
        filetypes=[("Image files", "*.jpg *.jpeg *.png *.bmp")]
    )
    return file_path

# ğŸ“¸ ÅÉ™klin yÃ¼klÉ™nmÉ™si
def load_image(image_path):
    if not os.path.exists(image_path):
        logging.error("ÅÉ™kil tapÄ±lmadÄ±: %s", image_path)
        raise FileNotFoundError("ÅÉ™kil tapÄ±lmadÄ±!")
    image = cv2.imread(image_path)
    logging.info("ÅÉ™kil yÃ¼klÉ™ndi: %s", image_path)
    return image

# ğŸ§  Ãœz aÅŸkarlanmasÄ± vÉ™ landmark Ã§Ä±xarÄ±lmasÄ±
def detect_face_landmarks(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    logging.info("Ãœz aÅŸkarlanmasÄ± baÅŸlayÄ±r...")
    
    # dlib-in hazÄ±r modellÉ™ri
    detector = dlib.get_frontal_face_detector()
    predictor = dlib.shape_predictor(CONFIG["shape_predictor_model"])

    faces = detector(gray)
    if len(faces) == 0:
        logging.warning("Ãœz tapÄ±lmadÄ±!")
        return None, None

    face = faces[0]
    shape = predictor(gray, face)

    landmarks = []
    for i in range(68):
        x = shape.part(i).x
        y = shape.part(i).y
        landmarks.append((x, y))

    logging.info("Ãœz tapÄ±ldÄ±, 68 landmark Ã§Ä±xarÄ±ldÄ±.")
    return face, landmarks

# ğŸ–¼ Landmark-larÄ± Ã§É™k (debug Ã¼Ã§Ã¼n)
def draw_landmarks(image, landmarks):
    for (x, y) in landmarks:
        cv2.circle(image, (x, y), 1, (0, 255, 0), -1)
    return image

# ğŸ” GÃ¶stÉ™rilÉ™n ÅŸÉ™kli test Ã¼Ã§Ã¼n ekranda gÃ¶stÉ™r (yalnÄ±z istÉ™yÉ™ baÄŸlÄ±)
def preview_image(image, window_name="Preview"):
    resized = cv2.resize(image, (600, 600))
    cv2.imshow(window_name, resized)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

# âœ¨ Bu hissÉ™ istifadÉ™ Ã¼Ã§Ã¼n funksiyaya salÄ±nÄ±r
def process_input_image(image_path):
    image = load_image(image_path)
    face, landmarks = detect_face_landmarks(image)
    if landmarks is None:
        raise ValueError("Ãœz tapÄ±lmadÄ±, baÅŸqa ÅŸÉ™kil sÄ±nayÄ±n.")
    
    logging.info("Ãœz uÄŸurla analiz edildi.")
    return image, face, landmarks
   
  # ================================
# Part 3: AddÄ±m-addÄ±m rÉ™sm Ã§É™kiliÅŸi vÉ™ frame-lÉ™r
# ================================

# ğŸ¨ Yeni boÅŸ kÉ™tan yarat (rÉ™sm Ã¼Ã§Ã¼n)
def create_blank_canvas(image, color=(255, 255, 255)):
    height, width = image.shape[:2]
    canvas = np.full((height, width, 3), color, dtype=np.uint8)
    return canvas

# âœï¸ Landmark-lardan xÉ™ttlÉ™rlÉ™ Ã¼z Ã§É™k
def draw_face_step_by_step(landmarks, canvas, save_dir="outputs/frames"):
    logging.info("RÉ™sm Ã§É™kmÉ™ prosesi baÅŸlayÄ±r...")
    step = 0
    drawn_points = []

    # Ä°stiqamÉ™tli Ã§É™kiliÅŸ Ã¼Ã§Ã¼n É™laqÉ™ xÉ™ritÉ™si
    connections = [
        list(range(0, 17)),      # Ã‡É™nÉ™ xÉ™tti
        list(range(17, 22)),     # SaÄŸ qaÅŸ
        list(range(22, 27)),     # Sol qaÅŸ
        list(range(27, 31)),     # Burun Ã¼st hissÉ™si
        list(range(31, 36)),     # Burun alt hissÉ™si
        list(range(36, 42)) + [36],  # SaÄŸ gÃ¶z
        list(range(42, 48)) + [42],  # Sol gÃ¶z
        list(range(48, 60)) + [48],  # Dodaqlar
        list(range(60, 68)) + [60]   # Daxili dodaq
    ]

    for group in connections:
        for i in range(len(group) - 1):
            pt1 = landmarks[group[i]]
            pt2 = landmarks[group[i + 1]]
            cv2.line(canvas, pt1, pt2, (0, 0, 0), CONFIG["line_thickness"])

            # ğŸ–¼ï¸ Frame yadda saxla
            frame_path = os.path.join(save_dir, f"frame_{step:03d}.png")
            cv2.imwrite(frame_path, canvas)
            step += 1
            drawn_points.append((pt1, pt2))

            logging.debug("XÉ™tt Ã§É™kildi: %s -> %s", pt1, pt2)

    logging.info("RÉ™sm Ã§É™kiliÅŸi tamamlandÄ±. Toplam %d xÉ™tt Ã§É™kildi.", len(drawn_points))
    return canvas, drawn_points

# ğŸ”‚ RÉ™ngli versiya (stil Ã¼Ã§Ã¼n)
def draw_colored_lines(landmarks, canvas, save_dir="outputs/frames_colored"):
    os.makedirs(save_dir, exist_ok=True)
    step = 0
    colors = [(255, 0, 0), (0, 128, 255), (0, 255, 128), (128, 0, 255), (0, 0, 0)]

    for i in range(len(landmarks) - 1):
        pt1 = landmarks[i]
        pt2 = landmarks[i + 1]
        color = random.choice(colors)
        cv2.line(canvas, pt1, pt2, color, CONFIG["line_thickness"] + 1)

        # Frame yadda saxla
        frame_path = os.path.join(save_dir, f"frame_{step:03d}.png")
        cv2.imwrite(frame_path, canvas)
        step += 1

    return canvas
    # ================================
# Part 4: Stil Transferi vÉ™ Vizual EffektlÉ™r
# ================================

# ğŸ¨ Stil transferi (OpenCV)
def apply_stylization_opencv(image):
    logging.info("OpenCV ilÉ™ stil transferi tÉ™tbiq olunur...")
    stylized_image = cv2.stylization(image, sigma_s=60, sigma_r=0.5)
    return stylized_image

# âœ¨ Karikatura effekti (optional artistik seÃ§im)
def apply_cartoon_effect(image):
    logging.info("Karikatura effekti tÉ™tbiq olunur...")
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    gray = cv2.medianBlur(gray, 7)

    edges = cv2.adaptiveThreshold(gray, 255,
                                  cv2.ADAPTIVE_THRESH_MEAN_C,
                                  cv2.THRESH_BINARY, 9, 10)

    color = cv2.bilateralFilter(image, 9, 300, 300)
    cartoon = cv2.bitwise_and(color, color, mask=edges)
    return cartoon

# ğŸ­ Torch ilÉ™ stil transferi (É™gÉ™r torch quraÅŸdÄ±rÄ±lÄ±bsa)
def apply_stylization_torch(content_img, style_img_path):
    try:
        import torchvision.transforms as transforms
        from torchvision.models import vgg19
        import torch.nn as nn
        import torch

        logging.info("PyTorch ilÉ™ stil transferi baÅŸlayÄ±r...")

        # ğŸ–¼ï¸ Stil ÅŸÉ™kli yÃ¼klÉ™
        style_img = Image.open(style_img_path).convert("RGB")
        transform = transforms.Compose([
            transforms.Resize((512, 512)),
            transforms.ToTensor()
        ])

        content_tensor = transform(content_img).unsqueeze(0)
        style_tensor = transform(style_img).unsqueeze(0)

        # ğŸ’¡ SadÉ™lÉ™ÅŸdirilmiÅŸ stil transferi modeli (sÃ¼rÉ™tli versiya)
        class StyleTransferNet(nn.Module):
            def __init__(self):
                super(StyleTransferNet, self).__init__()
                self.vgg = vgg19(pretrained=True).features[:21]
                for param in self.vgg.parameters():
                    param.requires_grad = False

            def forward(self, x):
                return self.vgg(x)

        model = StyleTransferNet().eval()
        with torch.no_grad():
            features = model(content_tensor)
        output = transforms.ToPILImage()(content_tensor.squeeze())
        return np.array(output)

    except ImportError:
        logging.error("Torch vÉ™ torchvision quraÅŸdÄ±rÄ±lmayÄ±b.")
        raise
        # ================================
# Part 5: Frame-lÉ™rdÉ™n Video vÉ™ PDF Ã§Ä±xÄ±ÅŸÄ±
# ================================

# ğŸ¥ Frame-lÉ™rdÉ™n video yarat
def create_video_from_frames(frame_dir="outputs/frames", output_path="outputs/drawing_video.mp4", fps=5):
    logging.info("Frame-lÉ™rdÉ™n video yaradÄ±lÄ±r: %s", output_path)

    images = sorted([img for img in os.listdir(frame_dir) if img.endswith(".png")])
    if not images:
        logging.warning("HeÃ§ bir frame tapÄ±lmadÄ±.")
        return

    first_frame = cv2.imread(os.path.join(frame_dir, images[0]))
    height, width, _ = first_frame.shape

    video_writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))

    for img_name in images:
        frame = cv2.imread(os.path.join(frame_dir, img_name))
        video_writer.write(frame)

    video_writer.release()
    logging.info("Video uÄŸurla yaradÄ±ldÄ±.")

# ğŸ“ PDF raport yarad
def create_pdf_report(original_image_path, final_drawing_path, stylized_path=None, pdf_path="outputs/drawing_report.pdf"):
    logging.info("PDF raport yaradÄ±lÄ±r...")

    pdf = FPDF()
    pdf.set_auto_page_break(auto=True, margin=15)
    pdf.add_page()
    pdf.set_font("Arial", size=14, style='B')
    pdf.cell(200, 10, txt="AI ilÉ™ RÉ™sm Ã‡É™kiliÅŸi Raportu", ln=True, align='C')
    pdf.ln(10)

    def add_image_section(title, image_path):
        if os.path.exists(image_path):
            pdf.set_font("Arial", size=12, style='B')
            pdf.cell(200, 10, txt=title, ln=True)
            pdf.image(image_path, w=180)
            pdf.ln(10)
        else:
            pdf.cell(200, 10, txt=f"{title} mÃ¶vcud deyil", ln=True)

    add_image_section("ğŸ“· Æsas ÅŸÉ™kil", original_image_path)
    add_image_section("ğŸ¨ Æl ilÉ™ Ã§É™kilmiÅŸ Ã¼z", final_drawing_path)

    if stylized_path:
        add_image_section("ğŸ–Œï¸ Stil transfer edilmiÅŸ rÉ™sim", stylized_path)

    pdf.output(pdf_path)
    logging.info("PDF uÄŸurla yaradÄ±ldÄ±: %s", pdf_path)
    # ================================
# Part 6: Æsas iÅŸ axÄ±nÄ± funksiyasÄ±
# ================================

def process_image_pipeline(image_path, style_path=None):
    try:
        os.makedirs("outputs", exist_ok=True)
        os.makedirs("outputs/frames", exist_ok=True)

        # ğŸ“¥ ÅÉ™kli yÃ¼klÉ™
        image = cv2.imread(image_path)
        if image is None:
            logging.error("ÅÉ™kil oxunmadÄ±: %s", image_path)
            return

        face_coords = detect_face(image)
        if face_coords is None:
            print("[X] ÅÉ™kildÉ™ Ã¼z tapÄ±lmadÄ±.")
            return

        face_crop = crop_to_face(image, face_coords)

        # ğŸ§  AI ilÉ™ Ã¼z analiz vÉ™ sadÉ™lÉ™ÅŸdirmÉ™
        simplified_face = stylize_with_mediapipe(face_crop)

        # âœï¸ AddÄ±m-addÄ±m Ã§É™kim frame-lÉ™ri
        simulate_drawing(simplified_face, output_dir="outputs/frames")

        # ğŸ¨ Stil transferi (OpenCV)
        stylized_img = apply_stylization_opencv(simplified_face)
        stylized_path = "outputs/stylized_face.png"
        cv2.imwrite(stylized_path, stylized_img)

        # ğŸ¥ Video dÃ¼zÉ™lt
        create_video_from_frames(frame_dir="outputs/frames", output_path="outputs/drawing_video.mp4", fps=5)

        # ğŸ“„ PDF Ã§Ä±xÄ±ÅŸÄ±
        create_pdf_report(
            original_image_path=image_path,
            final_drawing_path="outputs/frames/frame_final.png",
            stylized_path=stylized_path,
            pdf_path="outputs/drawing_report.pdf"
        )

        print("âœ… LayihÉ™ tamamlandÄ±. Ã‡Ä±xÄ±ÅŸlar:")
        print("   â€¢ Video: outputs/drawing_video.mp4")
        print("   â€¢ PDF: outputs/drawing_report.pdf")
        print("   â€¢ Stil ÅŸÉ™kli: outputs/stylized_face.png")

    except Exception as e:
        logging.exception("Emal zamanÄ± xÉ™ta baÅŸ verdi: %s", e)

# ğŸŸ¢ CLI vÉ™ ya interaktiv iÅŸlÉ™tmÉ™k Ã¼Ã§Ã¼n
if __name__ == "__main__":
    print("ğŸ¯ AI ilÉ™ Ãœz ÅÉ™kli Ã‡É™kiliÅŸi SisteminÉ™ xoÅŸ gÉ™lmisiniz")
    input_path = input("ÅÉ™klin yolunu daxil edin (mÉ™s: images/portrait.jpg): ").strip()
    if not os.path.isfile(input_path):
        print("Fayl mÃ¶vcud deyil.")
    else:
        use_style = input("Stil ÅŸÉ™kli É™lavÉ™ etmÉ™k istÉ™yirsÉ™n? (y/n): ").strip().lower()
        if use_style == 'y':
            style_path = input("Stil ÅŸÉ™kli yolunu daxil et: ").strip()
            process_image_pipeline(input_path, style_path)
        else:
            process_image_pipeline(input_path)
   # ================================
# Part 7: Helper funksiyalar vÉ™ sÉ™nÉ™dlÉ™ÅŸdirmÉ™
# ================================

def get_image_resolution(image):
    """
    ÅÉ™klin Ã¶lÃ§Ã¼lÉ™rini qaytarÄ±r (geniÅŸlik, hÃ¼ndÃ¼rlÃ¼k)
    """
    height, width = image.shape[:2]
    return width, height

def save_image_with_metadata(image, path, metadata="AI Generated Drawing"):
    """
    ÅÉ™kli metainfo ilÉ™ birlikdÉ™ yadda saxlayÄ±r
    """
    success = cv2.imwrite(path, image)
    if success:
        logging.info("ÅÉ™kil metadata ilÉ™ yadda saxlanÄ±ldÄ±: %s", path)
    else:
        logging.warning("ÅÉ™kli yadda saxlamaq mÃ¼mkÃ¼n olmadÄ±: %s", path)

def draw_bounding_box(image, coords, color=(0, 255, 0), thickness=2):
    """
    ÅÉ™kildÉ™ Ã¼z koordinatlarÄ±nÄ± gÃ¶stÉ™rÉ™n Ã§É™rÃ§ivÉ™ Ã§É™kir
    """
    (x, y, w, h) = coords
    return cv2.rectangle(image.copy(), (x, y), (x + w, y + h), color, thickness)

def clean_outputs_folder():
    """
    Ã‡Ä±xÄ±ÅŸ qovluqlarÄ±nÄ± tÉ™mizlÉ™yir
    """
    import shutil
    if os.path.exists("outputs"):
        shutil.rmtree("outputs")
        logging.info("outputs/ qovluÄŸu tÉ™mizlÉ™ndi.")

def resize_and_center_image(image, size=(512, 512)):
    """
    ÅÉ™kli verilmiÅŸ Ã¶lÃ§Ã¼lÉ™rÉ™ uyÄŸunlaÅŸdÄ±rÄ±b ortalayÄ±r
    """
    resized = cv2.resize(image, size)
    canvas = np.ones((size[1], size[0], 3), dtype=np.uint8) * 255
    canvas[:resized.shape[0], :resized.shape[1]] = resized
    return canvas

# ================================
# BONUS: ASCII sÉ™nÉ™tÉ™ Ã§evirmÉ™k (fun optional)
# ================================

def convert_to_ascii_art(image, cols=100, scale=0.5):
    """
    SadÉ™lÉ™ÅŸdirilmiÅŸ ÅŸÉ™kli ASCII sÉ™nÉ™tÉ™ Ã§evirir
    """
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    height, width = gray.shape
    cell_width = width / cols
    cell_height = cell_width / scale
    rows = int(height / cell_height)

    ascii_chars = "@%#*+=-:. "
    result = ""

    for i in range(rows):
        for j in range(cols):
            x1 = int(j * cell_width)
            y1 = int(i * cell_height)
            x2 = int((j + 1) * cell_width)
            y2 = int((i + 1) * cell_height)

            roi = gray[y1:y2, x1:x2]
            if roi.size == 0:
                result += " "
                continue

            avg = int(np.mean(roi))
            result += ascii_chars[int((avg * 9) / 255)]
        result += "\n"

    return result

# ================================
# Extra: ASCII sÉ™nÉ™ti yazdÄ±rmaq
# ================================

def export_ascii_to_txt(ascii_str, txt_path="outputs/ascii_art.txt"):
    """
    ASCII sÉ™nÉ™tini fayla yazdÄ±rÄ±r
    """
    with open(txt_path, "w") as f:
        f.write(ascii_str)
    logging.info("ASCII sÉ™nÉ™ti fayla yazÄ±ldÄ±: %s", txt_path)